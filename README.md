# liBERTy

liBERTyフレームワークは、日本語における少数及び長文のデータを用いたBERTによる自然言語処理に関連するライブラリ集です。

ここでは、特に文章の二値クラス分類について扱っており、少数データへのアプローチとしてのData Augmentation手法、長文データへのアプローチとしてのEnsemble学習手法について紹介しています。

これら2つを組み合わせることで少数データの日本語長文の自然言語処理が可能となります。


## Data Augmentation

日本語のData Augmentationルーチンを構築する上で、Easy Data Augmentation (EDA) を参考とした。

EDAでは，Random Deletion，Random Swap, Synonym Replacement, Random Insertion Random Deletionの4つのData Augmentation手法が示されている

- Random Delete および Swapは乱数によって指定した単語の削除や入れ替えによりデータのかさ増しを行う
- Synonym Replacement及びRandom Insertionは、一般に外部の同義語辞書をライブラリとしてインポートしそれを活用する手法がとられる。一般にインターネットで公開されているコードで参照している同義語辞書は2022年12月現在、使用不可である。仮にこの同義語辞書が入手できたとしても、BERTにおけるTokenizer処理により分散表現された単語に対するReplacementとなるように処理しなければならない。ここでは、BERTの類似語提供機能を用いてランダムな位置の単語を同義語に変更する。  
同義語置換では、まずランダムに同義語置換を行う対象となる文字を選び出し、別途同義語を求めるRobertモデルを用いて同義語を取得し入れ替える。BERTのマスク補完を用い、マスク配列で文字が存在する所に対して同義語置換する。マスク値が[MASK]つまり6である単語列を対象に置換する。  
- Random Insertionは、文中に出現する単語の同義語をランダムに挿入する。文脈を変えずにその文章に単語を増やす意図で用いられている。ここで用いる単語は文章に関連のある語彙である必要があり、このために同義語辞書を用いる必要がある。ここでは、本来文章中に出現する単語の同義語を挿入すべきところを、文章中に出現する単語を複製して挿入するという手法を用いている。文脈を変えずに文章に単語を増やすという意図をそのままに、外部ツールに頼らずデータのかさ増しを行うことが出来るようにした。

なお，ここで行うすべてのData Augmentationについて，文末の空白部分や，文の始まり及び接続部に登場する[CLS] [SEP]トークンに対して処理を行うことがないように工夫する必要がある．

### EDAとの違い

EDAと同様の方針で実装したがEDAではその実装方針について詳細が記されていない。この実装における実装方針は次のとおりである。

EDAの4つのData Augmentation手段において、変換後も元々存在した単語のみで構成される手段と、元々存在しない単語が混入する手段がある。EDAでは、置換と挿入は同義語を利用し、削除と入れ替えは同義語を利用しない実装である。つまり、削除と入れ替えは，元の文章の語彙を変化させることはなく、置換と挿入は類義語を利用するため語彙が変化する。EDAそのものの実装では、同義語は文章中の単語を利用する実装をオリジナルとしており、この場合はどの場合も語彙不変である．ここでは、Robertの同義語モデルを用いるため、語彙可変となる。

頻度指定の手法について、BERTの入力単語数、つまり入力ノード数当たりの置換単語数による指定と、乱数値が指定値よりも小さい場合に置換処理する確率的な指定方法がある。どちらも長期観測において置換確率は等しいが、局所的には差がある。単語数で指定する場合、BERTの入力単語数で切り出された部分文章それぞれについて等しく2個ずつ文字を入れ替えることができるが、確率的な指定手法ではある部分に集中して文字が入れ替わるような状況が発生し、置換場所の分散が異なる。このように部分等割合で対象を選ぶ手法と、全体等確率で対象を選ぶ手法がある。EDA置換・挿入・入れ替えが部分等割合、削除が全体等確率で実装されている。

例えば，ある部分に置換が集中するような同義語置換は、元のデータからの差分が大きくなる可能性があり、安定した結果を得るとは言い難いと考えれば、回数指定により結果的に安定的にファインチューニングが可能であると考えられる。このような方針で実装することも考えたが、EDAとの比較評価を行うという観点からEDA同様の実装方針を選択することとした。EDAのように文中から単に単語を選択するだけの類義語補完の効果がどの程度優れているのかが不明で確認する必要がある。

EDAは例えば、類似語を利用する2つの手法、すなわち類似語置換および類似語挿入は処理内容が似ている。共に別途類似語辞書や学習モデルを利用する場合語彙可変となる。これらを区別して評価可能とするため、あえて挿入については文中から単語を選んで補う語彙不変の手法とし、置換について別途学習モデルから単語を選んで入れ替える語彙可変の手法とした。この2つについて顕著な差が現れれば，語彙可変，語彙不変のどちらが有利かが判断できる。

## 長文への対応

BERTは一定文字数以上の長文を一度に処理することが出来ず、このキャパシティを超える長文を全てBERTに学習させるため、Ensemble学習を利用する。ここでは3つのEnsemble手法を構築するが、すべてStacking手法に分類される。

- Majority Decision Ensemble (MDE)は、単純な多数決を用いる。長文をブロックに切り分け、各ブロックにおいて推論して得られた結果にargmax関数をかけることでブロックごとに推論ラベルを付する。そのラベルの総数を計算し、より多かったラベルを最終的な長文全体のラベルとする。ここでは，Majority Decision Ensemble (MDE)と呼ぶ．MDEは全てのブロックを等価に扱うことで最終的な評価を得る手法であり、文章のどの部分にも均一に重要な文章が含まれている場合に効果的に分類できると考えられる。

- Network score Addition Ensemble (NAE)は、発展的な多数決手法であり、BERTによる推論を終えた結果を合計してからargmax関数をかける。MDEと異なり、argmax関数をかける前の細かな推論データが反映できる。ここで合計するのはargmax関数をかける前の値であるため、各ブロックにおける推定時の確度の大きさがそのまま評価されることになる。一般に異なる入力データの間でニューラルネットワークが出力する値の大小について比較できるかどうかは議論が必要である。また、Mean Square Errorロスではなくクロスエントロピーロスが利用されるため、その値で大小比較可能かどうかは確認が必要である。

- Softmax Addition Ensemble (SAE)は、BERTによる推論を終えた結果に対し、softmax関数の値を求めてから合計し、その後argmax関数をかける手法で、softmaxを用いて0から1の間の値で表現し、あたかも確率のように扱ったうえで合計する。softmax関数をかけた後の値であるため、重みの幅がクリップされているのと同様の効果を得ることができる。一般に異なる入力データの間で，ニューラルネットワークが出力する値の大小について比較出来ない場合があるが、softmax関数は広く表現手段として利用されており、受け入れられやすいと考えられる。

## ens-augens-re

Data Augmentationの効果を評価するコードで、よくある長文の一部を利用して分類する手法を利用する。

設定可能なコマンドラインハイパーパラメータオプションは次の通り。

- 学習に利用する文章数(-lオプション)  
学習時に利用する文章データセットについて，そこから学習データとして利用する文章の数を指定する．
- 検証に利用する文章数(-vオプション)  
通常はデータセットのうち，学習データとして利用した文章の残り全てを検証データとして利用する．すると，検証データが多くなり，また検証によって検証データの数が異なるため，実行時間短縮および検証のため，さらに検証データとして利用するデータ量を制限することができる．
-	エポック数(-eオプション)  
学習におけるエポック数を指定する．なお，ロス曲線により，評価に支障がないと判断され，かつ，過学習ではないエポック数として20を初期値として与えている．
-	バッチサイズ(-bオプション)  
ミニバッチサイズを指定する．大きくすることで実行処理時間の短縮に寄与するが，GPUメモリサイズが増える．64としている．
-	利用する記事の種類(-aオプション)  
利用する記事を指定する．本研究では日本語を利用した自然言語処理で頻繁に利用されるlivedoorニュースコーパスを利用する．  
分類上平均的な精度が期待できる記事「dokujo-tsushin」と「it-life-hack」のクラス分け，および，記事の特徴が類似しており分類が比較的困難な「dokujo-tsushin」と「peachy」のクラス分けを選択可能とした．
-	日本語文章処理Data Augmentation(-tオプション)  
日本語に対応するData Augmentation手段を構築したが，それらの利用を指定することができる．なお，複数の利用も可能とするため，各Data Augmentation手段を表現した文字の組み合わせで指定する．synonym replace(r)，random insert(i)，random delete(d)，random swap(s)で指定し，例えばsynonym replaceとrandom deleteを利用する場合は’rd’と指定する．
-	日本語文章処理Data Augmentation(-pオプション)  
評価では，文章の先頭を単語区切りで利用する手法(h)，文章の末尾を単語区切りで利用する手法(t)，文章の先頭を文字区切りで利用する手法(a)，文章の末尾を文字区切りで利用する手法(k)，文章の末尾を文字区切りで利用しつつ，最後の部分に記者名が含まれるためそれを削除する手法(K)，要約文章を用いる手法(s)の6種類の評価を行う．これを個別に指定することができる．例えば，先頭を利用する2つの手法を用いる場合は’ha’と指定する．なお，複数の手法を選択した場合は，自動的にアンサンブル手法としてMD法による評価も算出される．
-	synonym replaceの回数(-rオプション)  
synonym replaceの回数を整数で指定する．
-	random insertの回数(-iオプション)  
random insertの回数を整数で指定する．
-	random deleteの割合(-dオプション)  
random deleteの割合を0から1の小数で指定する．
-	random swapの回数(-sオプション)  
random swapの回数を整数で指定する．

なお，別途-fオプションが存在するが，これはjupyter notebookで起動されたか，コマンドラインで起動されたかを区別するためのオプションであり，通常は指定しない．

## ens-augens-slide

長い文章を区切りながらBERTに入力してアンサンブル手法で識別する手法を用いるコードで、3つのシンプルなアンサンブル手法を利用して評価する。オプションは ens-augens-reと同様。

## ens-augens-lgbm

評価用のため、特にオプションは設けていない。
上のslideで作った全文を蓄えたデータセットを用いてLightGBMで分類する。比較評価用。
