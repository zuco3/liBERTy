# liBERTy

## ens-augens-re
- わかち済み文章の頭何文字(head)
- わかち済み文章のおしり何文字(tail)
- わかちしていない文章の頭何文字(atama)
- わかちしていない文章のおしり何文字(ketsu)
- わかちしていない文章のおしり何文字マイナス10文字(kketsu)
- 要約BERTを通したやつ(summary)

が入っているやつ。

>精度が悪い？ので、教師データの数を調整しながらいいところを見る。Data Augmentationについての比較評価もこれのデータローダのTransformerをいじって検証すればいいはず。

## ens-augens-slide
長い文章を区切りながらBERTに突っ込んでいくやつ。

下の方のHOUHOU1とか2とかってところでアンサンブル学習をしようとしている。HOUHOU1がargmaxをかけてから多数決、HOUHOU2が出力の総和を計算してからargmaxを取っている。

>学習がどこかで狂ってるのか、推論が全部0か1になってしまう。argmaxをかける前の結果を見ても全部似たりよったりな値しか出てない。多分どっかミスってる。

## ens-augens-lgbm
上のslideで作った全文を蓄えたデータセットをコピって作ったLGBMだけのやつ。これは問題なく動いているはず。

>教師データの数が足りないマンなので変更して検証する必要あり？
