# liBERTy

liBERTyフレームワークは、日本語における少数及び長文のデータを用いたBERTによる自然言語処理に関連するライブラリ集です。

ここでは、特に文章の二値クラス分類について扱っており、少数データへのアプローチとしてのData Augmentation手法、長文データへのアプローチとしてのEnsemble学習手法について紹介しています。

これら2つを組み合わせることで少数データの日本語長文の自然言語処理が可能となります。


## Data Augmentation

日本語のData Augmentationルーチンを構築する上で、Easy Data Augmentation (EDA) を参考とした。

EDAでは，Random Deletion，Random Swap, Synonym Replacement, Random Insertion Random Deletionの4つのData Augmentation手法が示されている

- Random Delete および Swapは乱数によって指定した単語の削除や入れ替えによりデータのかさ増しを行う
- Synonym Replacement及びRandom Insertionは、一般に外部の同義語辞書をライブラリとしてインポートしそれを活用する手法がとられる。一般にインターネットで公開されているコードで参照している同義語辞書は2022年12月現在、使用不可である。仮にこの同義語辞書が入手できたとしても、BERTにおけるTokenizer処理により分散表現された単語に対するReplacementとなるように処理しなければならない。ここでは、BERTの類似語提供機能を用いてランダムな位置の単語を同義語に変更する。
- Random Insertionは、文中に出現する単語の同義語をランダムに挿入する。文脈を変えずにその文章に単語を増やす意図で用いられている。ここで用いる単語は文章に関連のある語彙である必要があり、このために同義語辞書を用いる必要がある。ここでは、本来文章中に出現する単語の同義語を挿入すべきところを、文章中に出現する単語を複製して挿入するという手法を用いている。文脈を変えずに文章に単語を増やすという意図をそのままに、外部ツールに頼らずデータのかさ増しを行うことが出来るようにした。

なお，ここで行うすべてのData Augmentationについて，文末の空白部分や，文の始まり及び接続部に登場する[CLS] [SEP]トークンに対して処理を行うことがないように工夫する必要がある．

## 長文への対応

BERTは一定文字数以上の長文を一度に処理することが出来ず、このキャパシティを超える長文を全てBERTに学習させるため、Ensemble学習を利用する。ここでは3つのEnsemble手法を構築するが、すべてStacking手法に分類される。

- Majority Decision Ensemble (MDE)は、単純な多数決を用いる。長文をブロックに切り分け、各ブロックにおいて推論して得られた結果にargmax関数をかけることでブロックごとに推論ラベルを付する。そのラベルの総数を計算し、より多かったラベルを最終的な長文全体のラベルとする。ここでは，Majority Decision Ensemble (MDE)と呼ぶ．MDEは全てのブロックを等価に扱うことで最終的な評価を得る手法であり、文章のどの部分にも均一に重要な文章が含まれている場合に効果的に分類できると考えられる。

- Network score Addition Ensemble (NAE)は、発展的な多数決手法であり、BERTによる推論を終えた結果を合計してからargmax関数をかける。MDEと異なり、argmax関数をかける前の細かな推論データが反映できる。ここで合計するのはargmax関数をかける前の値であるため、各ブロックにおける推定時の確度の大きさがそのまま評価されることになる。一般に異なる入力データの間でニューラルネットワークが出力する値の大小について比較できるかどうかは議論が必要である。また、Mean Square Errorロスではなくクロスエントロピーロスが利用されるため、その値で大小比較可能かどうかは確認が必要である。

- Softmax Addition Ensemble (SAE)は、BERTによる推論を終えた結果に対し、softmax関数の値を求めてから合計し、その後argmax関数をかける手法で、softmaxを用いて0から1の間の値で表現し、あたかも確率のように扱ったうえで合計する。softmax関数をかけた後の値であるため、重みの幅がクリップされているのと同様の効果を得ることができる。一般に異なる入力データの間で，ニューラルネットワークが出力する値の大小について比較出来ない場合があるが、softmax関数は広く表現手段として利用されており、受け入れられやすいと考えられる。


## ens-augens-re
- わかち済み文章の頭何文字(head)
- わかち済み文章のおしり何文字(tail)
- わかちしていない文章の頭何文字(atama)
- わかちしていない文章のおしり何文字(ketsu)
- わかちしていない文章のおしり何文字マイナス10文字(kketsu)
- 要約BERTを通したやつ(summary)

が入っているやつ。

>精度が悪い？ので、教師データの数を調整しながらいいところを見る。Data Augmentationについての比較評価もこれのデータローダのTransformerをいじって検証すればいいはず。

## ens-augens-slide
長い文章を区切りながらBERTに突っ込んでいくやつ。

下の方のHOUHOU1とか2とかってところでアンサンブル学習をしようとしている。HOUHOU1がargmaxをかけてから多数決、HOUHOU2が出力の総和を計算してからargmaxを取っている。

>学習がどこかで狂ってるのか、推論が全部0か1になってしまう。argmaxをかける前の結果を見ても全部似たりよったりな値しか出てない。多分どっかミスってる。

## ens-augens-lgbm
上のslideで作った全文を蓄えたデータセットをコピって作ったLGBMだけのやつ。これは問題なく動いているはず。

>教師データの数が足りないマンなので変更して検証する必要あり？
